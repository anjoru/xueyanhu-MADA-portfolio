---
title: "fitting exercise"
---

### Load required packages

```{r}
library(dplyr)
library(purrr)
library(ggplot2)
library(here)
library(tidyr)
library(tibble)
library(tidymodels)
library(parsnip)
library(yardstick)
library(pROC)
library(caret)
library(class)
library(parsnip)
```

### Load the dataset

```{r}
data_location <- here::here("fitting-exercise","data","raw-data","Mavoglurant_A2121_nmpk.csv")
mydata <- read.csv(data_location)
```

### create plot of DV by time with ID as a group factor

```{r}
# Plotting using ggplot2
p1 <- ggplot(mydata, aes(x = TIME, y = DV, group = ID, color = factor(DOSE))) +
  geom_line() +
  labs(title = "Line Plot of DY and time by ID Stratified by Dose",
       x = "Time",
       y = "DV") +
  scale_color_manual(values = c("25" = "blue", "37.5" = "green", "50" = "red")) +
  theme_minimal()
plot(p1)
```

### data cleaning

```{r}
# Filter rows where OCC is equal to 1
d1 <- mydata %>%
  filter(OCC == 1)

# Sum DV of each ID
d2 <- d1 %>%
  filter(TIME != 0) %>%  # Exclude observations with TIME = 0
  group_by(ID) %>%
  summarise(Y = sum(DV))

# Data frame only include TIME == 0
d3 <- d1 %>%
  filter(TIME == 0)

# Combine data frames d2 and d3 together
d4 <- left_join(d2, d3, by = "ID")

# A little bit more data cleaning
d5 <- d4 %>%
  mutate(SEX = factor(SEX),
         RACE = factor(RACE)) %>%
  select(Y, DOSE, AGE, SEX, RACE, WT, HT)

# Save the cleaned data
save_data_location <- here::here("fitting-exercise","data","processed-data","processeddata.rds")
saveRDS(d5, file = save_data_location)
```

### Do a few more of the exploratory process

```{r}
# Summary table
summary_df = skimr::skim(d5)
print(summary_df)

# A bar chart of total drug by age

d5_p1 <- d5 %>%
  group_by(AGE) %>%
  summarize(avg_drug_level = mean(Y))

p1 <- ggplot(d5_p1, aes(x = AGE, y = avg_drug_level)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
  labs(title = "Bar chart of Total Drug Level vs Age",
       x = "Age",
       y = "Total Drug Level") +
  theme_minimal()
plot(p1)

# scatterplot of total drug by weight

p2 <- ggplot(d5, aes(x = WT, y = Y)) +
  geom_point() +
  labs(title = "Scatterplot of Total Drug Level vs Weight",
       x = "Weight",
       y = "Total Drug Level") +
  theme_minimal()
plot(p2)

# boxplot of total drug by dose level
p3 <- ggplot(d5, aes(x = as.factor(DOSE), y = Y)) +
  geom_boxplot() +
  labs(title = "Boxplot of Total Drug Level vs Dose",
       x = "Dose Level",
       y = "Total Drug Level") +
  theme_minimal()
plot(p3)

# boxplot of total drug by sex

p4 <- ggplot(d5, aes(x = as.factor(SEX), y = Y)) +
  geom_boxplot() +
  labs(title = "Boxplot of Total Drug Level vs Sex",
       x = "Sex",
       y = "Total Drug Level") +
  theme_minimal()
plot(p4)

# bar chart of total drug by race

d5_p5 <- d5 %>%
  group_by(RACE) %>%
  summarize(
    mean_Y = mean(Y),
    sd_Y = sd(Y)
  )  # Calculate mean and standard deviation for each group


p5 <- ggplot(d5_p5, aes(x = RACE, y = mean_Y, fill = RACE)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.7) +
  geom_errorbar(aes(ymin = mean_Y - sd_Y, ymax = mean_Y + sd_Y),
                position = position_dodge(width = 0.9), width = 0.2) +
  labs(title = "Bar Chart with Error Bars of Total Drug Level vs Race",
       x = "Race",
       y = "Mean Total Drug Level") +
  theme_minimal()  # Create the bar chart with error bars
plot(p5)
```

There is not a clear correlation between total drug level and age nor weight. And it is quite obvious that individuals with higher dose level will have higher total drug level. While the mean total drug level for male is higher than female (assumeing odd number represents male and even number represents female). Also, the race identified as number 7 has the lowest mean total drug level among 4 different races.

### Model fitting

#### Linear model to the continuous outcome (Y)

```{r}
# DOSE as a factor
d5$DOSE <- as.factor(d5$DOSE)

# define the linear model specification
lm_mod_dose <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") 
lm_mod_all <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") 

# split the data
set.seed(123)
data_split5 <- initial_split(d5, prop = 3/4)
train_data5 <- training(data_split5)
test_data5  <- testing(data_split5)

# create recipe
lm_rec_dose <- recipe(Y ~ DOSE, data=train_data5)
lm_rec_all <- recipe(Y ~ ., data=train_data5)

# create workflow
lm_wf_dose <- workflow() %>%
  add_model(lm_mod_dose) %>%
  add_recipe(lm_rec_dose)

lm_wf_all <- workflow() %>%
  add_model(lm_mod_all) %>%
  add_recipe(lm_rec_all)

# train the model
lm_fit_dose <- fit(lm_wf_dose, data = train_data5)

lm_fit_all <- fit(lm_wf_all, data = train_data5)

# Make predictions on the test data
test_predictions_dose <- predict(lm_fit_dose, new_data = test_data5) %>%
  bind_cols(test_data5)
test_predictions_all <- predict(lm_fit_all, new_data = test_data5) %>%
  bind_cols(test_data5)

# Calculate RMSE
rmse_dose <- rmse(test_predictions_dose, truth = Y,estimate = .pred)
rmse_all <- rmse(test_predictions_all, truth = Y,estimate = .pred)
cat("RMSE for DOSE:", rmse_dose$.estimate, "\n")
cat("RMSE for all predictors:", rmse_all$.estimate, "\n")

# Calculate R-squared
rsquared_dose <- rsq(test_predictions_dose, truth = Y, estimate = .pred)
rsquared_all <- rsq(test_predictions_all, truth = Y, estimate = .pred)
cat("Rsquare for DOSE:", rsquared_dose$.estimate, "\n")
cat("Rsquare for all predictors:", rsquared_all$.estimate, "\n")
```

RMSE for dose model is smaller than that for all predictors model, so dose model performs better on metric RMSE. While R-square for dose model is small than all predictors model, and the latter is closer to 1, which means all predictors model performs better on metric R-square. Therefore, the performance of models may be difference based on the chosen metric.

#### Linear model for SEX as the outcome of interest

```{r}
d5$SEX <- as.factor(d5$SEX)
# logistic specification
lg_mod_dose <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")
lg_mod_all <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

# data splitting
set.seed(123)
data_split5 <- initial_split(d5, prop = 3/4)
train_data5 <- training(data_split5)
test_data5  <- testing(data_split5)

# create workflow
lg_wf_dose <-
workflow() %>%
  add_model(lg_mod_dose) %>%
  add_formula(SEX ~ DOSE)
lg_wf_all <-
workflow() %>%
  add_model(lg_mod_all) %>%
  add_formula(SEX ~ .)

lg_fit_dose <- fit(lg_wf_dose, data = train_data5)
lg_fit_all <- fit(lg_wf_all, data = train_data5)

lg_training_pred_dose <- 
  predict(lg_fit_dose, train_data5) %>% 
  bind_cols(predict(lg_fit_dose, train_data5, type = "prob")) %>% 
  # Add the true outcome data back in
  bind_cols(train_data5 %>% 
              select(SEX))
lg_training_pred_all <- 
  predict(lg_fit_all, train_data5) %>% 
  bind_cols(predict(lg_fit_all, train_data5, type = "prob")) %>% 
  bind_cols(train_data5 %>% 
              select(SEX))

# get roc_auc and accuracy
roc_auc_dose <- lg_training_pred_dose %>% # training set predictions
  roc_auc(truth = SEX, .pred_1)
print(roc_auc_dose)
roc_auc_all <- lg_training_pred_all %>% # training set predictions
  roc_auc(truth = SEX, .pred_1)
print(roc_auc_all)

accuracy_dose <- lg_training_pred_dose %>%  # training set predictions
  accuracy(truth = SEX, .pred_class)
print(accuracy_dose)
accuracy_all <- lg_training_pred_all %>%  # training set predictions
  accuracy(truth = SEX, .pred_class)
print(accuracy_all)
```

```{r}
# Create a logistic regression model for SEX and all the predictors
logistic_model2 <- glm(SEX ~ Y + DOSE + RACE + WT + HT, family = "binomial", data = d5) 

# Print the summary of the model
summary(logistic_model2)

# Obtain predicted probabilities from the logistic regression model
predicted_probs2 <- predict(logistic_model2, type = "response")

# Convert predicted probabilities to binary predictions (0 or 1)
predicted_class2 <- ifelse(predicted_probs2 > 0.5, 1, 0)

# Convert SEX to a binary factor (0 and 1)
d5$SEX <- as.factor(ifelse(d5$SEX == 1, 0, 1))
predicted_classes <- factor(predicted_class2)

# Create a tibble for metrics
metrics_tbl2 <- tibble(truth = d5$SEX, estimate = as.factor(predicted_classes)) 

# Create confusion matrix
conf_matrix <- confusionMatrix(predicted_classes, d5$SEX)

# Extract accuracy
accuracy_value2 <- conf_matrix$overall["Accuracy"]

# Print accuracy
print(accuracy_value2)

# Create ROC curve
roc_curve2 <- roc(d5$SEX, predicted_probs2)

# Calculate AUC
roc_auc2 <- auc(roc_curve2)

# Print ROC-AUC
print(roc_auc2)
```

The logistic regression model 1 suggests that, based on the provided predictors (DOSE), there isn't strong evidence to support a significant relationship with the outcome variable SEX. I found the accuracy for this model is 0. I tried to fix the code with AI's help but didn't really work. So I guess maybe it's because the model's predictions are not accurate? And AUC is 0.5919 which might not be providing strong discrimination but might distinguish between the positive and negative classes is somewhat better than random chance based on my searching. For the rm2, HT is statistically significant predictors of SEX while others aren't. Accuracy at 0.958 indicates the model is well-fitted, and AUC shows the excellent discrimination.

#### try *k*-nearest neighbors model

```{r}
# Split data into training and testing sets
set.seed(123)  # for reproducibility
sample_indices <- sample(nrow(d5), 0.7 * nrow(d5))
train_data <- d5[sample_indices, ]
test_data <- d5[-sample_indices, ]

# Fit k-nearest neighbors model
k_value <- 3  # Set the desired value for k
knn_model <- knn(train = train_data[, "DOSE", drop = FALSE], 
                 test = test_data[, "DOSE", drop = FALSE], 
                 cl = train_data$Y, k = k_value)

# Evaluate the model
# Assuming 'test_data$Y' contains the true outcomes in the test set
confusion_matrix <- table(Actual = test_data$Y, Predicted = knn_model)
print(confusion_matrix)
```

## Exercise 10

### Part 1

#### data preperation

```{r}
# select variables
d6 <- d5 %>%
  select(Y, DOSE, AGE, SEX, WT, HT)

# set a seed
rngseed = 1234
set.seed(rngseed)

# Put 3/4 of the data into the training set 
data_split <- initial_split(d6, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

#### fit linear model only using training set

```{r}
# model specification
lr_mod1 <- linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")
lr_mod2 <- linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")

# create recipe
lm_rec1 <- recipe(Y~DOSE,data=train_data)
lm_rec2 <- recipe(Y~.,data=train_data)

# combine model and recipe to workflow
lr_wf1 <- workflow() %>%
  add_model(lr_mod1) %>%
  add_recipe(lm_rec1)
lr_wf2 <- workflow() %>%
  add_model(lr_mod2) %>%
  add_recipe(lm_rec2)

# fit the workflows to the training data
lr_fit1 <- fit(lr_wf1, data = train_data)
lr_fit2 <- fit(lr_wf2, data = train_data)

# Make predictions and calculate RMSE
preds1 <- predict(lr_fit1, new_data = train_data) %>%
  bind_cols(train_data)
preds2 <- predict(lr_fit2, new_data = train_data) %>%
  bind_cols(train_data)
rmse1 <- rmse(preds1, truth = Y, estimate = .pred)
rmse2 <- rmse(preds2, truth = Y, estimate = .pred)

cat("RMSE for DOSE:", rmse1$.estimate, "\n")
cat("RMSE for ALL PREDICTORS:", rmse2$.estimate, "\n")
```

#### Model performance assessment 1

```{r}
# Calculate the mean outcome
mean_outcome <- mean(train_data$Y)

# Predict the mean outcome for all observations
predicted_values <- rep(mean_outcome, nrow(train_data))

# Calculate the RMSE (Root Mean Squared Error)
rmse <- sqrt(mean((train_data$Y - predicted_values)^2))

# Print the RMSE
cat("RMSE:", rmse, "\n")
```

#### Model performance assessment 2

##### use the same seed and run CV

```{r}
#reset seed
rngseed = 1234
set.seed(rngseed)

# create 10-fold cross-validation folds
new_folds <- vfold_cv(train_data, v = 10)

# define model specification
linear_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# workflow
linear_wf1 <- workflow() %>%
  add_model(linear_spec) %>%
  add_formula(Y ~ DOSE)

linear_wf2 <- workflow() %>%
  add_model(linear_spec) %>%
  add_formula(Y ~ .)

# perform CV model training and evaluation
cv_results1 <- linear_wf1 %>%
  fit_resamples(new_folds)
cv_results2 <- linear_wf2 %>%
  fit_resamples(new_folds)

# collect RMSE values
collect_metrics(cv_results1)
collect_metrics(cv_results2)
```
For both models, the RMSE calculated after cross-validation is applied to the data is higher than the fitting model at the first time which is obtained without using CV. I think it may means CV could reduce the overfitting of the model using the whole training dataset.

#### set a new seed and re-run CV
```{r}
#reset seed
set.seed(2345)

# create 10-fold cross-validation folds
re_new_folds <- vfold_cv(train_data, v = 10)

# define model specification
re_linear_spec <- linear_reg() %>%
  set_engine("lm") %>%
  set_mode("regression")

# workflow
re_linear_wf1 <- workflow() %>%
  add_model(re_linear_spec) %>%
  add_formula(Y ~ DOSE)

re_linear_wf2 <- workflow() %>%
  add_model(linear_spec) %>%
  add_formula(Y ~ .)

# perform CV model training and evaluation
re_cv_results1 <- fit_resamples(
  re_linear_wf1,
  re_new_folds
)

re_cv_results2 <- fit_resamples(
  re_linear_wf2,
  re_new_folds
)

# Print the structure of collected metrics
collect_metrics(re_cv_results1)
collect_metrics(re_cv_results2)
```
Each new RMSE after reseting the seed is lower than first CV is applied but higher than first fitting model. I think it still shows the effect of CV for reducing overfitting. But there is another point that the metrics may change regard how subsets are created by CV.

